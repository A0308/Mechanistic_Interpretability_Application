{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0f8333ad66c94157a31412a01d5e07a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0f0d654572484ddb9b074043e6bcc833",
              "IPY_MODEL_76ba24dcc1b84908abcb0fb9699c91d4",
              "IPY_MODEL_15a4b578a326410fb46ea99c5f513d12"
            ],
            "layout": "IPY_MODEL_80575774bb384bdca852a7e3594afbae"
          }
        },
        "0f0d654572484ddb9b074043e6bcc833": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35333c84ab054d588281ef50ac737f0e",
            "placeholder": "​",
            "style": "IPY_MODEL_fc5d195273d64c459a73737306353d41",
            "value": "100%"
          }
        },
        "76ba24dcc1b84908abcb0fb9699c91d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_665a86db86ce438081e973b06ecaef37",
            "max": 4000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_805befa224c54c01bf9ee852e17bd540",
            "value": 4000
          }
        },
        "15a4b578a326410fb46ea99c5f513d12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_814d170b788b461094ae56d03eea7cd8",
            "placeholder": "​",
            "style": "IPY_MODEL_a94dcae6496442249a79b34436cc5dd3",
            "value": " 4000/4000 [07:44&lt;00:00,  7.28it/s]"
          }
        },
        "80575774bb384bdca852a7e3594afbae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35333c84ab054d588281ef50ac737f0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc5d195273d64c459a73737306353d41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "665a86db86ce438081e973b06ecaef37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "805befa224c54c01bf9ee852e17bd540": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "814d170b788b461094ae56d03eea7cd8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a94dcae6496442249a79b34436cc5dd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "Keqkmk1z4Jvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
        "DEBUG_MODE = False\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "    %pip install git+https://github.com/neelnanda-io/TransformerLens.git\n",
        "    # Install another version of node that makes PySvelte work way faster\n",
        "    !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
        "    %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
        "    # Needed for PySvelte to work, v3 came out and broke things...\n",
        "    %pip install typeguard==2.13.3\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hcbma9re4KxF",
        "outputId": "66b6ca7e-8819-4caf-cf78-18e1a4a52327"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running as a Colab notebook\n",
            "Collecting git+https://github.com/neelnanda-io/TransformerLens.git\n",
            "  Cloning https://github.com/neelnanda-io/TransformerLens.git to /tmp/pip-req-build-tci3e3x0\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/TransformerLens.git /tmp/pip-req-build-tci3e3x0\n",
            "  Resolved https://github.com/neelnanda-io/TransformerLens.git to commit 760135a27c4b7873b0cb66aca541958f8939f60b\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting accelerate>=0.23.0 (from transformer-lens==0.0.0)\n",
            "  Downloading accelerate-0.29.2-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.4/297.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting beartype<0.15.0,>=0.14.1 (from transformer-lens==0.0.0)\n",
            "  Downloading beartype-0.14.1-py3-none-any.whl (739 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.7/739.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting better-abc<0.0.4,>=0.0.3 (from transformer-lens==0.0.0)\n",
            "  Downloading better_abc-0.0.3-py3-none-any.whl (3.5 kB)\n",
            "Collecting datasets>=2.7.1 (from transformer-lens==0.0.0)\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops>=0.6.0 (from transformer-lens==0.0.0)\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fancy-einsum>=0.0.3 (from transformer-lens==0.0.0)\n",
            "  Downloading fancy_einsum-0.0.3-py3-none-any.whl (6.2 kB)\n",
            "Collecting jaxtyping>=0.2.11 (from transformer-lens==0.0.0)\n",
            "  Downloading jaxtyping-0.2.28-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.7/40.7 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.24 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (2.0.3)\n",
            "Requirement already satisfied: rich>=12.6.0 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (13.7.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (0.1.99)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.66.2)\n",
            "Requirement already satisfied: transformers>=4.37.2 in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.38.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from transformer-lens==0.0.0) (4.10.0)\n",
            "Collecting wandb>=0.13.5 (from transformer-lens==0.0.0)\n",
            "  Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.23.0->transformer-lens==0.0.0) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.13.3)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.7.1->transformer-lens==0.0.0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2.31.0)\n",
            "Collecting xxhash (from datasets>=2.7.1->transformer-lens==0.0.0)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=2.7.1->transformer-lens==0.0.0)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.7.1->transformer-lens==0.0.0) (3.9.3)\n",
            "Collecting typeguard==2.13.3 (from jaxtyping>=0.2.11->transformer-lens==0.0.0)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.5->transformer-lens==0.0.0) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.6.0->transformer-lens==0.0.0) (2.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10->transformer-lens==0.0.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10->transformer-lens==0.0.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10->transformer-lens==0.0.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10->transformer-lens==0.0.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10->transformer-lens==0.0.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10->transformer-lens==0.0.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10->transformer-lens==0.0.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10->transformer-lens==0.0.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10->transformer-lens==0.0.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10->transformer-lens==0.0.0)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10->transformer-lens==0.0.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->transformer-lens==0.0.0) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10->transformer-lens==0.0.0)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer-lens==0.0.0) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->transformer-lens==0.0.0) (0.15.2)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n",
            "  Downloading sentry_sdk-1.45.0-py2.py3-none-any.whl (267 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb>=0.13.5->transformer-lens==0.0.0)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting setproctitle (from wandb>=0.13.5->transformer-lens==0.0.0)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb>=0.13.5->transformer-lens==0.0.0) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb>=0.13.5->transformer-lens==0.0.0) (1.16.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.7.1->transformer-lens==0.0.0) (4.0.3)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.6.0->transformer-lens==0.0.0) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets>=2.7.1->transformer-lens==0.0.0) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->transformer-lens==0.0.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->transformer-lens==0.0.0) (1.3.0)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb>=0.13.5->transformer-lens==0.0.0)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: transformer-lens\n",
            "  Building wheel for transformer-lens (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformer-lens: filename=transformer_lens-0.0.0-py3-none-any.whl size=129881 sha256=36abcd42f0ebfb0c5c3fe12ed37a9cc96c9810a291176f052888e6b1a65b1641\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6spi4b3r/wheels/8a/1e/37/ffb9c15454a1725b13a9d9f5e74fb91725048884ad734b8c1f\n",
            "Successfully built transformer-lens\n",
            "Installing collected packages: better-abc, xxhash, typeguard, smmap, setproctitle, sentry-sdk, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fancy-einsum, einops, docker-pycreds, dill, beartype, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, jaxtyping, gitdb, nvidia-cusolver-cu12, GitPython, wandb, datasets, accelerate, transformer-lens\n",
            "Successfully installed GitPython-3.1.43 accelerate-0.29.2 beartype-0.14.1 better-abc-0.0.3 datasets-2.18.0 dill-0.3.8 docker-pycreds-0.4.0 einops-0.7.0 fancy-einsum-0.0.3 gitdb-4.0.11 jaxtyping-0.2.28 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 sentry-sdk-1.45.0 setproctitle-1.3.3 smmap-5.0.1 transformer-lens-0.0.0 typeguard-2.13.3 wandb-0.16.6 xxhash-3.4.1\n",
            "\n",
            "\u001b[31m================================================================================\u001b[m\n",
            "\u001b[31m================================================================================\u001b[m\n",
            "\n",
            "  \u001b[1m\u001b[33m                            DEPRECATION WARNING                            \u001b[m\n",
            "\n",
            "    \u001b[1m\u001b[4m Node.js 16.x is no longer actively supported!\u001b[m\n",
            "\n",
            "  \u001b[1mYou will not receive security or critical stability updates\u001b[m for this version.\n",
            "\n",
            "  You should migrate to a supported version of Node.js as soon as possible.\n",
            "  Use the installation script that corresponds to the version of Node.js you\n",
            "  wish to install. e.g.\n",
            "  \n",
            "   * \u001b[31mhttps://deb.nodesource.com/setup_16.x — Node.js 16 \"Gallium\" \u001b[1m(deprecated)\u001b[m\n",
            "   * \u001b[32mhttps://deb.nodesource.com/setup_18.x — Node.js 18 \"Hydrogen\" (Maintenance)\u001b[m\n",
            "   * \u001b[31mhttps://deb.nodesource.com/setup_19.x — Node.js 19 \"Nineteen\" \u001b[1m(deprecated)\u001b[m\n",
            "   * \u001b[1m\u001b[32mhttps://deb.nodesource.com/setup_20.x — Node.js 20 LTS \"Iron\" (recommended)\u001b[m\n",
            "   * \u001b[32mhttps://deb.nodesource.com/setup_21.x — Node.js 21 \"Iron\" (current)\u001b[m\n",
            "   \n",
            "\n",
            "\n",
            "  Please see \u001b[1mhttps://github.com/nodejs/Release\u001b[m for details about which\n",
            "  version may be appropriate for you.\n",
            "\n",
            "  The \u001b[32m\u001b[1mNodeSource\u001b[m Node.js distributions repository contains\n",
            "  information both about supported versions of Node.js and supported Linux\n",
            "  distributions. To learn more about usage, see the repository:\n",
            "   \u001b[4m\u001b[1mhttps://github.com/nodesource/distributions\u001b[m\n",
            "\n",
            "\u001b[31m================================================================================\u001b[m\n",
            "\u001b[31m================================================================================\u001b[m\n",
            "\n",
            "Continuing in 10 seconds ...\n",
            "\n",
            "\u001b[38;5;79m2024-04-11 16:00:46 - Installing pre-requisites\u001b[0m\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,974 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,174 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,357 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,081 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,691 kB]\n",
            "Fetched 8,510 kB in 3s (3,179 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ca-certificates is already the newest version (20230311ubuntu0.22.04.1).\n",
            "curl is already the newest version (7.81.0-1ubuntu1.16).\n",
            "gnupg is already the newest version (2.2.27-3ubuntu2.1).\n",
            "gnupg set to manually installed.\n",
            "The following NEW packages will be installed:\n",
            "  apt-transport-https\n",
            "0 upgraded, 1 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 1,510 B of archives.\n",
            "After this operation, 170 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 apt-transport-https all 2.4.12 [1,510 B]\n",
            "Fetched 1,510 B in 0s (15.8 kB/s)\n",
            "Selecting previously unselected package apt-transport-https.\n",
            "(Reading database ... 121752 files and directories currently installed.)\n",
            "Preparing to unpack .../apt-transport-https_2.4.12_all.deb ...\n",
            "Unpacking apt-transport-https (2.4.12) ...\n",
            "Setting up apt-transport-https (2.4.12) ...\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Get:5 https://deb.nodesource.com/node_16.x nodistro InRelease [12.1 kB]\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 https://deb.nodesource.com/node_16.x nodistro/main amd64 Packages [7,253 B]\n",
            "Fetched 19.4 kB in 2s (9,112 B/s)\n",
            "Reading package lists... Done\n",
            "\u001b[1;32m2024-04-11 16:01:06 - Repository configured successfully. To install Node.js, run: apt-get install nodejs -y\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  nodejs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 50 not upgraded.\n",
            "Need to get 27.5 MB of archives.\n",
            "After this operation, 128 MB of additional disk space will be used.\n",
            "Get:1 https://deb.nodesource.com/node_16.x nodistro/main amd64 nodejs amd64 16.20.2-1nodesource1 [27.5 MB]\n",
            "Fetched 27.5 MB in 1s (38.1 MB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package nodejs.\n",
            "(Reading database ... 121756 files and directories currently installed.)\n",
            "Preparing to unpack .../nodejs_16.20.2-1nodesource1_amd64.deb ...\n",
            "Unpacking nodejs (16.20.2-1nodesource1) ...\n",
            "Setting up nodejs (16.20.2-1nodesource1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Collecting git+https://github.com/neelnanda-io/PySvelte.git\n",
            "  Cloning https://github.com/neelnanda-io/PySvelte.git to /tmp/pip-req-build-s2umwdcl\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/neelnanda-io/PySvelte.git /tmp/pip-req-build-s2umwdcl\n",
            "  Resolved https://github.com/neelnanda-io/PySvelte.git to commit 6f5d971a148d40fb7481d400ae74551b37340e83\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: typeguard==2.13.3 in /usr/local/lib/python3.10/dist-packages (2.13.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import plotly.io as pio\n",
        "\n",
        "if IN_COLAB or not DEBUG_MODE:\n",
        "    # Thanks to annoying rendering issues, Plotly graphics will either show up in colab OR Vscode depending on the renderer - this is bad for developing demos! Thus creating a debug mode.\n",
        "    pio.renderers.default = \"colab\"\n",
        "else:\n",
        "    pio.renderers.default = \"png\""
      ],
      "metadata": {
        "id": "P4jiDHht4SO6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import einops\n",
        "import tqdm.notebook as tqdm\n",
        "import random\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from jaxtyping import Float, Int\n",
        "from typing import List, Union, Optional\n",
        "from functools import partial\n",
        "import copy\n",
        "\n",
        "import itertools\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "import dataclasses\n",
        "from dataclasses import dataclass\n",
        "import datasets\n",
        "from IPython.display import HTML"
      ],
      "metadata": {
        "id": "2vK1fdIp4UPF"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import pysvelte\n",
        "\n",
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
      ],
      "metadata": {
        "id": "tu6S4jxg4Wq3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plotting helper functions\n",
        "\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "update_layout_set = {\"xaxis_range\", \"yaxis_range\", \"hovermode\", \"xaxis_title\", \"yaxis_title\", \"colorbar\", \"colorscale\", \"coloraxis\", \"title_x\", \"bargap\", \"bargroupgap\", \"xaxis_tickformat\", \"yaxis_tickformat\", \"title_y\", \"legend_title_text\", \"xaxis_showgrid\", \"xaxis_gridwidth\", \"xaxis_gridcolor\", \"yaxis_showgrid\", \"yaxis_gridwidth\"}\n",
        "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    if isinstance(tensor, list):\n",
        "        tensor = torch.stack(tensor)\n",
        "    kwargs_post = {k: v for k, v in kwargs.items() if k in update_layout_set}\n",
        "    kwargs_pre = {k: v for k, v in kwargs.items() if k not in update_layout_set}\n",
        "    if \"facet_labels\" in kwargs_pre:\n",
        "        facet_labels = kwargs_pre.pop(\"facet_labels\")\n",
        "    else:\n",
        "        facet_labels = None\n",
        "    if \"color_continuous_scale\" not in kwargs_pre:\n",
        "        kwargs_pre[\"color_continuous_scale\"] = \"RdBu\"\n",
        "    fig = px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0,labels={\"x\":xaxis, \"y\":yaxis}, **kwargs_pre).update_layout(**kwargs_post)\n",
        "    if facet_labels:\n",
        "        for i, label in enumerate(facet_labels):\n",
        "            fig.layout.annotations[i]['text'] = label\n",
        "\n",
        "    fig.show(renderer)\n",
        "\n",
        "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.line(y=utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
        "    x = utils.to_numpy(x)\n",
        "    y = utils.to_numpy(y)\n",
        "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)\n",
        "\n",
        "def lines(lines_list, x=None, mode='lines', labels=None, xaxis='', yaxis='', title = '', log_y=False, hover=None, **kwargs):\n",
        "    # Helper function to plot multiple lines\n",
        "    if type(lines_list)==torch.Tensor:\n",
        "        lines_list = [lines_list[i] for i in range(lines_list.shape[0])]\n",
        "    if x is None:\n",
        "        x=np.arange(len(lines_list[0]))\n",
        "    fig = go.Figure(layout={'title':title})\n",
        "    fig.update_xaxes(title=xaxis)\n",
        "    fig.update_yaxes(title=yaxis)\n",
        "    for c, line in enumerate(lines_list):\n",
        "        if type(line)==torch.Tensor:\n",
        "            line = utils.to_numpy(line)\n",
        "        if labels is not None:\n",
        "            label = labels[c]\n",
        "        else:\n",
        "            label = c\n",
        "        fig.add_trace(go.Scatter(x=x, y=line, mode=mode, name=label, hovertext=hover, **kwargs))\n",
        "    if log_y:\n",
        "        fig.update_layout(yaxis_type=\"log\")\n",
        "    fig.show()\n",
        "\n",
        "def bar(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.bar(\n",
        "        y=utils.to_numpy(tensor),\n",
        "        labels={\"x\": xaxis, \"y\": yaxis},\n",
        "        template=\"simple_white\",\n",
        "        **kwargs).show(renderer)"
      ],
      "metadata": {
        "id": "2sdrzxL94cBx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformer_lens.patching as patching\n",
        "from transformer_lens import evals\n",
        "import math"
      ],
      "metadata": {
        "id": "VnFVPg1o4edh"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_attn_patterns(heads, local_tokens, local_cache, title: str = \"\"):\n",
        "    labels = []\n",
        "    patterns = []\n",
        "    batch_index = 0\n",
        "\n",
        "    for head in heads:\n",
        "        if isinstance(head, tuple):\n",
        "            layer, head_index = head\n",
        "        else:\n",
        "            layer, head_index = head // model.cfg.n_heads, head % model.cfg.n_heads\n",
        "        patterns.append(local_cache[\"pattern\", layer][batch_index, head_index])\n",
        "        labels.append(f\"L{layer}H{head_index}\")\n",
        "    patterns = torch.stack(patterns, dim=-1)\n",
        "    attn_viz = pysvelte.AttentionMulti(tokens=model.to_str_tokens(local_tokens[batch_index]), attention=patterns, head_labels=labels)\n",
        "    display(HTML(f\"<h3>{title}</h3>\"))\n",
        "    attn_viz.show()"
      ],
      "metadata": {
        "id": "4Gp4HJur4hzK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Model"
      ],
      "metadata": {
        "id": "KELXi0944j8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_grad_enabled(True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6XX-Smw4mMN",
        "outputId": "79beab40-2756-404c-f149-654ad992fedd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x7fb611dff490>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mm36JkWi4oGA",
        "outputId": "5e09c983-eb79-4ab0-9922-8ac2b5d8d365"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LIST_LEN = 3\n",
        "MAX_INT = 50\n",
        "cfg = HookedTransformerConfig(\n",
        "    n_layers=2,\n",
        "    n_heads=1,\n",
        "    d_model=128,\n",
        "    d_head=128,\n",
        "    n_ctx=3*LIST_LEN+3, # BOS d1 d2 d3 MID p1 p2 p3 END a1 a2 a3\n",
        "    d_vocab= MAX_INT+3, # 0, ..., MAX_INT-1, BOS, MID, END\n",
        "    d_vocab_out=MAX_INT,\n",
        "    attn_only=True,\n",
        "    normalization_type=None,\n",
        "    device=device,\n",
        "    seed=0,\n",
        ")\n",
        "model = HookedTransformer(cfg)\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pqK71Z04rdk",
        "outputId": "f46b868f-66b6-4624-a7ad-5bc9578e761d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HookedTransformer(\n",
            "  (embed): Embed()\n",
            "  (hook_embed): HookPoint()\n",
            "  (pos_embed): PosEmbed()\n",
            "  (hook_pos_embed): HookPoint()\n",
            "  (blocks): ModuleList(\n",
            "    (0-1): 2 x TransformerBlock(\n",
            "      (ln1): Identity()\n",
            "      (attn): Attention(\n",
            "        (hook_k): HookPoint()\n",
            "        (hook_q): HookPoint()\n",
            "        (hook_v): HookPoint()\n",
            "        (hook_z): HookPoint()\n",
            "        (hook_attn_scores): HookPoint()\n",
            "        (hook_pattern): HookPoint()\n",
            "        (hook_result): HookPoint()\n",
            "      )\n",
            "      (hook_attn_in): HookPoint()\n",
            "      (hook_q_input): HookPoint()\n",
            "      (hook_k_input): HookPoint()\n",
            "      (hook_v_input): HookPoint()\n",
            "      (hook_mlp_in): HookPoint()\n",
            "      (hook_attn_out): HookPoint()\n",
            "      (hook_mlp_out): HookPoint()\n",
            "      (hook_resid_pre): HookPoint()\n",
            "      (hook_resid_post): HookPoint()\n",
            "    )\n",
            "  )\n",
            "  (unembed): Unembed()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# disable biases\n",
        "for name, param in model.named_parameters():\n",
        "    if 'b_' in name:\n",
        "        param.requires_grad = False\n",
        "    print(name, param.shape, param.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDX4Whyo4ugb",
        "outputId": "4c5a0bb5-db02-4553-d523-8252a606ed2a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "embed.W_E torch.Size([53, 128]) True\n",
            "pos_embed.W_pos torch.Size([12, 128]) True\n",
            "blocks.0.attn.W_Q torch.Size([1, 128, 128]) True\n",
            "blocks.0.attn.W_O torch.Size([1, 128, 128]) True\n",
            "blocks.0.attn.b_Q torch.Size([1, 128]) False\n",
            "blocks.0.attn.b_O torch.Size([128]) False\n",
            "blocks.0.attn.W_K torch.Size([1, 128, 128]) True\n",
            "blocks.0.attn.W_V torch.Size([1, 128, 128]) True\n",
            "blocks.0.attn.b_K torch.Size([1, 128]) False\n",
            "blocks.0.attn.b_V torch.Size([1, 128]) False\n",
            "blocks.1.attn.W_Q torch.Size([1, 128, 128]) True\n",
            "blocks.1.attn.W_O torch.Size([1, 128, 128]) True\n",
            "blocks.1.attn.b_Q torch.Size([1, 128]) False\n",
            "blocks.1.attn.b_O torch.Size([128]) False\n",
            "blocks.1.attn.W_K torch.Size([1, 128, 128]) True\n",
            "blocks.1.attn.W_V torch.Size([1, 128, 128]) True\n",
            "blocks.1.attn.b_K torch.Size([1, 128]) False\n",
            "blocks.1.attn.b_V torch.Size([1, 128]) False\n",
            "unembed.W_U torch.Size([128, 50]) True\n",
            "unembed.b_U torch.Size([50]) False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training"
      ],
      "metadata": {
        "id": "89GuzSGD4wtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task Dataset"
      ],
      "metadata": {
        "id": "Vug_1spC4xr_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_data_generator(cfg, batch_size, seed=0):\n",
        "    torch.manual_seed(seed)\n",
        "    BOS_TOKEN = cfg.d_vocab-1\n",
        "    MID_TOKEN = cfg.d_vocab-2\n",
        "    END_TOKEN = cfg.d_vocab-3\n",
        "    while True:\n",
        "        seq = torch.randint(0, MAX_INT, (batch_size, LIST_LEN))\n",
        "        perm = torch.randperm(LIST_LEN)\n",
        "        ans = seq[:, perm]\n",
        "\n",
        "        bos_tensor = einops.repeat(torch.tensor(BOS_TOKEN), \" -> i 1\", i=batch_size)\n",
        "        mid_tensor = einops.repeat(torch.tensor(MID_TOKEN), \" -> i 1\", i=batch_size)\n",
        "        end_tensor = einops.repeat(torch.tensor(END_TOKEN), \" -> i 1\", i=batch_size)\n",
        "\n",
        "        x = torch.cat([bos_tensor, seq, mid_tensor, einops.repeat(perm, \"seq -> batch seq\", batch=batch_size), end_tensor, ans], dim=-1)\n",
        "        yield x\n",
        "\n",
        "print(next(make_data_generator(cfg, 2)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1iZP-qQ40au",
        "outputId": "cb96bade-7d1b-4479-e539-decfda690031"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[52, 44, 39, 33, 51,  1,  2,  0, 50, 39, 33, 44],\n",
            "        [52, 10, 13, 29, 51,  1,  2,  0, 50, 13, 29, 10]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loss Function"
      ],
      "metadata": {
        "id": "37P00Z_043ZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_fn(logits, tokens):\n",
        "    logits=logits[:, -LIST_LEN-1:-1, :]\n",
        "    logits = logits.to(torch.float64)\n",
        "    labels = tokens[:, -LIST_LEN:]\n",
        "\n",
        "    log_probs = logits.log_softmax(dim=-1)\n",
        "    correct_log_probs = log_probs.gather(dim=-1, index=labels[..., None])[..., 0]\n",
        "    return -correct_log_probs.mean()\n",
        "\n",
        "with torch.no_grad():\n",
        "    tokens = next(make_data_generator(cfg, 2)).to(device)\n",
        "    logits = model(tokens)\n",
        "    loss = loss_fn(logits, tokens)\n",
        "    print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZIhlgjr453L",
        "outputId": "240c3925-6137-4ee5-9696-7169d8f67189"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(3.9640, dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('uniform loss:', np.log(cfg.d_vocab_out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6n0HMnIG48m9",
        "outputId": "70650970-8a4f-4936-a880-c2e180e779a5"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uniform loss: 3.912023005428146\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Setup Optimizer"
      ],
      "metadata": {
        "id": "qOlBAeSO5AKZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-3\n",
        "wd = 0.01\n",
        "betas = (0.9, 0.98)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd, betas=betas)\n",
        "\n",
        "batch_size = 256\n",
        "train_data_loader = make_data_generator(cfg, batch_size)"
      ],
      "metadata": {
        "id": "nnFdStz65DCE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Loop"
      ],
      "metadata": {
        "id": "CvJwUZyF5GDW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 4000\n",
        "\n",
        "train_losses = []\n",
        "for epoch in tqdm.tqdm(range(num_epochs)):\n",
        "    tokens = next(train_data_loader).to(device)\n",
        "    logits = model(tokens)\n",
        "    loss = loss_fn(logits, tokens)\n",
        "    loss.backward()\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch: {epoch}, train_loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743,
          "referenced_widgets": [
            "0f8333ad66c94157a31412a01d5e07a0",
            "0f0d654572484ddb9b074043e6bcc833",
            "76ba24dcc1b84908abcb0fb9699c91d4",
            "15a4b578a326410fb46ea99c5f513d12",
            "80575774bb384bdca852a7e3594afbae",
            "35333c84ab054d588281ef50ac737f0e",
            "fc5d195273d64c459a73737306353d41",
            "665a86db86ce438081e973b06ecaef37",
            "805befa224c54c01bf9ee852e17bd540",
            "814d170b788b461094ae56d03eea7cd8",
            "a94dcae6496442249a79b34436cc5dd3"
          ]
        },
        "id": "n6PSlT-L5IfM",
        "outputId": "ec2be684-348e-45a6-c2da-525d176fe5d0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f8333ad66c94157a31412a01d5e07a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, train_loss: 3.9126681423614014\n",
            "Epoch: 100, train_loss: 1.7503021757225596\n",
            "Epoch: 200, train_loss: 1.4704949486103251\n",
            "Epoch: 300, train_loss: 0.9829875616178234\n",
            "Epoch: 400, train_loss: 0.03023505979362966\n",
            "Epoch: 500, train_loss: 0.039463109395148056\n",
            "Epoch: 600, train_loss: 0.0006447695685105333\n",
            "Epoch: 700, train_loss: 0.022440187069656493\n",
            "Epoch: 800, train_loss: 0.0015573624023373274\n",
            "Epoch: 900, train_loss: 2.16064946622243e-05\n",
            "Epoch: 1000, train_loss: 0.003525535229202404\n",
            "Epoch: 1100, train_loss: 0.0010389910995369164\n",
            "Epoch: 1200, train_loss: 0.00015402106237967064\n",
            "Epoch: 1300, train_loss: 4.502118188786725e-05\n",
            "Epoch: 1400, train_loss: 3.591175651756231e-06\n",
            "Epoch: 1500, train_loss: 1.7859222722574152e-06\n",
            "Epoch: 1600, train_loss: 0.0031789599349202283\n",
            "Epoch: 1700, train_loss: 0.0004103817472111452\n",
            "Epoch: 1800, train_loss: 2.1507927352726425e-05\n",
            "Epoch: 1900, train_loss: 6.925996265165917e-06\n",
            "Epoch: 2000, train_loss: 3.628497620560638e-06\n",
            "Epoch: 2100, train_loss: 1.720199441067257e-06\n",
            "Epoch: 2200, train_loss: 3.7714526817813844e-07\n",
            "Epoch: 2300, train_loss: 0.001400841065644041\n",
            "Epoch: 2400, train_loss: 2.722962918243774e-05\n",
            "Epoch: 2500, train_loss: 1.0368907953770478e-05\n",
            "Epoch: 2600, train_loss: 1.3568710681232862e-05\n",
            "Epoch: 2700, train_loss: 3.402947136131837e-06\n",
            "Epoch: 2800, train_loss: 1.5205904053603248e-06\n",
            "Epoch: 2900, train_loss: 4.80100848089586e-07\n",
            "Epoch: 3000, train_loss: 3.715668538665914e-07\n",
            "Epoch: 3100, train_loss: 1.1295718774180562e-07\n",
            "Epoch: 3200, train_loss: 4.1181438029594786e-08\n",
            "Epoch: 3300, train_loss: 1.1484260555601611e-08\n",
            "Epoch: 3400, train_loss: 1.0524051959845269e-08\n",
            "Epoch: 3500, train_loss: 8.607948081213159e-06\n",
            "Epoch: 3600, train_loss: 1.907247075359765e-05\n",
            "Epoch: 3700, train_loss: 3.639560515790236e-05\n",
            "Epoch: 3800, train_loss: 2.527343980275627e-05\n",
            "Epoch: 3900, train_loss: 4.038518314944292e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "line(train_losses,\n",
        "     title=\"Loss curve\",\n",
        "     xaxis=\"Epoch\", yaxis=\"Loss\")"
      ],
      "metadata": {
        "id": "_qmTQRvB5L5d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sanity Check"
      ],
      "metadata": {
        "id": "L3p5TPCr5kGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get a test sample with multiple different permutations\n",
        "test_batch_size = 256\n",
        "test_data = []\n",
        "sub_batch_size = 4\n",
        "for i in range(test_batch_size // sub_batch_size):\n",
        "    test_data.append(next(make_data_generator(cfg, sub_batch_size, seed=i)))\n",
        "\n",
        "test_data = torch.cat(test_data, dim=0).to(device)\n",
        "print(test_data.shape)"
      ],
      "metadata": {
        "id": "lKl-0zOg5mWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "    test_logits = model(test_data)\n",
        "    test_logits = test_logits[:, -LIST_LEN-1:-1, :]\n",
        "    preds = test_logits.argmax(dim=-1)\n",
        "    test_labels = test_data[:, -LIST_LEN:]\n",
        "\n",
        "    acc = (preds == test_labels).float().mean()\n",
        "    print(\"Test sample accuracy:\", acc.item())"
      ],
      "metadata": {
        "id": "g-kJzW7n5oux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save Model"
      ],
      "metadata": {
        "id": "9AOYsMZJ5tFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir ../models"
      ],
      "metadata": {
        "id": "-PkEgsAD5uZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"../models/permute_lists_model.pt\"\n",
        "torch.save(model.state_dict(), filename)"
      ],
      "metadata": {
        "id": "88EuN9Hw50Ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check we can load in model\n",
        "LIST_LEN = 3\n",
        "MAX_INT = 50\n",
        "cfg = HookedTransformerConfig(\n",
        "    n_layers=2,\n",
        "    n_heads=1,\n",
        "    d_model=128,\n",
        "    d_head=128,\n",
        "    n_ctx=3*LIST_LEN+3, # BOS d1 d2 d3 MID p1 p2 p3 END a1 a2 a3\n",
        "    d_vocab= MAX_INT+3, # 0, ..., MAX_INT-1, BOS, MID, END\n",
        "    d_vocab_out=MAX_INT,\n",
        "    attn_only=True,\n",
        "    normalization_type=None,\n",
        "    device=device,\n",
        "    seed=0,\n",
        ")\n",
        "\n",
        "model_loaded = HookedTransformer(cfg)\n",
        "model.load_state_dict(torch.load(filename), strict=True)"
      ],
      "metadata": {
        "id": "HQAuWF4W58ZB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}